{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Project Attn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktAMHpzO2Pa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "from keras import backend as K\n",
        "from tensorflow.python.keras.layers import Layer, InputSpec\n",
        "from keras import initializers\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eqpskII9k9n",
        "outputId": "f5027426-2d44-459e-c2ff-22d9b2e1f2e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4SnGHl5j3X0"
      },
      "source": [
        "**TextAttnBiRNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaB7M_Demdeh",
        "outputId": "d20faa78-8a6a-4e31-bb57-b41f03117023"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  8 09:31:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R7yoMOYkNFu",
        "outputId": "c8535cfc-13e1-4614-ec6e-bc2a86f7cf3a"
      },
      "source": [
        "cd /content/gdrive/MyDrive/College/Semester5/ML/Project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/College/Semester5/ML/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYz1pecgk6_M"
      },
      "source": [
        "# !pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak07QYnM-QQC"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "            # 2\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence = Attention()(hidden)\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
        "                                 shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
        "                                     shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
        "        if self.bias:\n",
        "            e += self.b\n",
        "        e = K.tanh(e)\n",
        "\n",
        "        a = K.exp(e)\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        a = K.expand_dims(a)\n",
        "\n",
        "        print(a)\n",
        "        c = K.sum(a * x, axis=1)\n",
        "        return c\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.features_dim\n",
        "\n",
        "    def cal_att_weights(output, att_w):\n",
        "        #if model_name == 'HAN':\n",
        "        eij = np.tanh(np.dot(output[0], att_w[0]) + att_w[1])\n",
        "        eij = np.dot(eij, att_w[2])\n",
        "        eij = eij.reshape((eij.shape[0], eij.shape[1]))\n",
        "        ai = np.exp(eij)\n",
        "        weights = ai / np.sum(ai)\n",
        "\n",
        "return weights\n",
        "from keras import backend as K\n",
        "sent_before_att = K.function([model1.layers[0].input,K.learning_phase()],  [model1.layers[2].output])\n",
        "sent_att_w = model1.layers[5].get_weights()\n",
        "test_seq=np.array(userinp)\n",
        "test_seq=np.array(test_seq).reshape(1,118,100)\n",
        "out = sent_before_att([test_seq, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYDReXrA-K8f"
      },
      "source": [
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Embedding, Dense, Bidirectional\n",
        "from keras.layers import GRU\n",
        "import tensorflow.keras.layers\n",
        "from tensorflow.keras import layers\n",
        "class TextAttBiRNN(Model):\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(TextAttBiRNN, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
        "        self.bi_rnn = Bidirectional(GRU(128, return_sequences=True))  # LSTM or GRU\n",
        "        # self.conv_layer = layers.Convolution1D(3, 2, activation=\"relu\")\n",
        "        self.conv_layer = layers.Convolution1D(30, 2, activation=\"relu\", padding = 'same')\n",
        "        self.conv_layer2 = layers.Convolution1D(30, 3, activation=\"relu\", padding = 'same')\n",
        "        # self.pooling_layer = layers.GlobalMaxPool1D(keepdims = True)\n",
        "        self.input_layer = tf.keras.Input(self.maxlen)\n",
        "        self.attention = Attention(self.maxlen)\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of TextAttBiRNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError('The maxlen of inputs of TextAttBiRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        x = self.input_layer(inputs)\n",
        "        embedding = self.embedding(x)\n",
        "        x = self.conv_layer(embedding)\n",
        "        x = self.conv_layer2(x)\n",
        "        # x = self.pooling_layer(x)\n",
        "        x = self.bi_rnn(x)\n",
        "        x = self.attention(x)\n",
        "        # print(x.shape)\n",
        "        output = self.classifier(x)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbRfG9ckUlS",
        "outputId": "e8006826-2d08-45a1-e910-9ed5c14c7f30"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn import metrics\n",
        "max_features = 5000\n",
        "maxlen = 40\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "\n",
        "\n",
        "print('Loading data...')\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/College/Semester5/ML/Project/twitter16/twitter16.csv\")\n",
        "df = df.rename(columns = {'source_tweet':'message', 'label':'class'})\n",
        "df = df.sample(frac=1)\n",
        "train = df[:int(0.80*df.shape[0])]\n",
        "test = df[int(0.80*df.shape[0]):]\n",
        "print(train.columns)\n",
        "print(test.columns)\n",
        "x_train, y_train, x_test, y_test = train['message'], train['class'], test['message'], test['class']\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)...')\n",
        "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('/content/gdrive/MyDrive/Amazon/productner/data/glove.6B.100d.txt')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train['message'])\n",
        "word_index = token.word_index\n",
        "\n",
        "x_train = sequence.pad_sequences(token.texts_to_sequences(x_train), maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(token.texts_to_sequences(x_test), maxlen=maxlen)\n",
        "\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Index(['tweetID', 'message', 'class', 'tree'], dtype='object')\n",
            "Index(['tweetID', 'message', 'class', 'tree'], dtype='object')\n",
            "328 train sequences\n",
            "83 test sequences\n",
            "Pad sequences (samples x time)...\n",
            "x_train shape: (328, 40)\n",
            "x_test shape: (83, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EB9wUe1tdrt",
        "outputId": "0f45d486-597f-4cc3-b61d-9708aacbd9b2"
      },
      "source": [
        "epochs = 20\n",
        "print('Build model...')\n",
        "model = TextAttBiRNN(maxlen, max_features, embedding_dims)\n",
        "model.compile('adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train...')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, mode='max')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[early_stopping],\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "print('Test...')\n",
        "result = model.predict(x_test)"
      ],
      "metadata": {
        "id": "SDuRRYgjvupJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTtckxNYsxeK"
      },
      "source": [
        "result = (result.reshape(1,-1)[0]>0.5).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgHSj7O-j-v3",
        "outputId": "830f5276-ed13-4bd0-9cd6-cf5f3a8fd3fd"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wlmM9imrFp_",
        "outputId": "75611cb9-e80e-4bc5-9bf7-8ba855b852e7"
      },
      "source": [
        "print(\"ACC\", metrics.accuracy_score(result, y_test))\n",
        "print(\"Prec\", metrics.precision_score(result, y_test))\n",
        "print(\"REC\", metrics.recall_score(result, y_test))\n",
        "print(\"F1\", metrics.f1_score(result, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC 0.9036144578313253\n",
            "Prec 0.9230769230769231\n",
            "REC 0.8780487804878049\n",
            "F1 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pocyY-6ZRmZx"
      },
      "source": [
        "**Saving Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQEe-cLDQdc7"
      },
      "source": [
        "model.save_weights(\"_hcan_\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHOqhaoORp0_"
      },
      "source": [
        "**Load Saved Model weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNpko48jRLbL"
      },
      "source": [
        "model2 = HCAN(maxlen_sentence, maxlen_word, max_features, embedding_dims)\n",
        "model2.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYhCd_rVQzzk",
        "outputId": "6616c06d-8c84-44f2-e723-7ce3ddfe9aed"
      },
      "source": [
        "model2.load_weights(\"_hcan_\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbebf93de50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tgMG1JpRRaY"
      },
      "source": [
        "result = model2.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rBbAY6yRUkR",
        "outputId": "0d7ceaa8-c07a-4b10-d0de-d99eef5beec4"
      },
      "source": [
        "result = (result.reshape(1,-1)[0]>0.5).astype(int)\n",
        "print(\"ACC\", metrics.accuracy_score(result, y_test))\n",
        "print(\"Prec\", metrics.precision_score(result, y_test))\n",
        "print(\"REC\", metrics.recall_score(result, y_test))\n",
        "print(\"F1\", metrics.f1_score(result, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC 0.9824519230769231\n",
            "Prec 0.9863272041489863\n",
            "REC 0.9794007490636704\n",
            "F1 0.982851773549448\n"
          ]
        }
      ]
    }
  ]
}